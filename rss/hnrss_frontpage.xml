<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>黑客新闻：头版</title>
    <link>https://news.ycombinator.com/</link>
    <description>黑客新闻 RSS</description>
    <lastBuildDate>Fri, 07 Nov 2025 01:27:40 GMT</lastBuildDate>
    <item>
      <title>科学家找到增强衰老大脑记忆力的方法</title>
      <link>https://news.vt.edu/articles/2025/10/cals-jarome-improving-memory.html</link>
      <description><![CDATA[文章网址：https://news.vt.edu/articles/2025/10/cals-jarome-improving-memory.html
评论网址：https://news.ycombinator.com/item?id=45842263
积分：10
# 条评论：1]]></description>
      <guid>https://news.vt.edu/articles/2025/10/cals-jarome-improving-memory.html</guid>
      <pubDate>Fri, 07 Nov 2025 00:22:53 GMT</pubDate>
    </item>
    <item>
      <title>游戏设计很简单</title>
      <link>https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/</link>
      <description><![CDATA[文章网址：https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/
评论网址：https://news.ycombinator.com/item?id=45841262
积分：86
# 条评论：29]]></description>
      <guid>https://www.raphkoster.com/2025/11/03/game-design-is-simple-actually/</guid>
      <pubDate>Thu, 06 Nov 2025 22:24:23 GMT</pubDate>
    </item>
    <item>
      <title>学习循环和法学硕士</title>
      <link>https://martinfowler.com/articles/llm-learning-loop.html</link>
      <description><![CDATA[文章网址：https://martinfowler.com/articles/llm-learning-loop.html
评论网址：https://news.ycombinator.com/item?id=45841056
积分：90
# 条评论：57]]></description>
      <guid>https://martinfowler.com/articles/llm-learning-loop.html</guid>
      <pubDate>Thu, 06 Nov 2025 22:05:58 GMT</pubDate>
    </item>
    <item>
      <title>Hightouch (YC S19) 正在招聘</title>
      <link>https://job-boards.greenhouse.io/hightouch/jobs/5542602004</link>
      <description><![CDATA[文章网址：https://job-boards.greenhouse.io/hightouch/jobs/5542602004
评论网址：https://news.ycombinator.com/item?id=45840612
积分：0
# 条评论：0]]></description>
      <guid>https://job-boards.greenhouse.io/hightouch/jobs/5542602004</guid>
      <pubDate>Thu, 06 Nov 2025 21:23:46 GMT</pubDate>
    </item>
    <item>
      <title>宇宙的膨胀“现在正在放缓，而不是加速”</title>
      <link>https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding</link>
      <description><![CDATA[文章网址：https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding
评论网址：https://news.ycombinator.com/item?id=45840200
积分：83
评论数：88]]></description>
      <guid>https://ras.ac.uk/news-and-press/research-highlights/universes-expansion-now-slowing-not-speeding</guid>
      <pubDate>Thu, 06 Nov 2025 20:45:39 GMT</pubDate>
    </item>
    <item>
      <title>你应该写一个代理</title>
      <link>https://fly.io/blog/everyone-write-an-agent/</link>
      <description><![CDATA[文章网址：https://fly.io/blog/everyone-write-an-agent/
评论网址：https://news.ycombinator.com/item?id=45840088
积分：250
评论数：127]]></description>
      <guid>https://fly.io/blog/everyone-write-an-agent/</guid>
      <pubDate>Thu, 06 Nov 2025 20:37:06 GMT</pubDate>
    </item>
    <item>
      <title>20 亿电子邮件地址被泄露</title>
      <link>https://www.troyhunt.com/2-billion-email-addresses-were-exposed-and-we-indexed-them-all-in-have-i-been-pwned/</link>
      <description><![CDATA[文章网址：https://www.troyhunt.com/2-billion-email-addresses-were-expose-and-we-indexed-them-all-in-have-i-been-pwned/
评论网址：https://news.ycombinator.com/item?id=45839901
积分：313
# 条评论：223]]></description>
      <guid>https://www.troyhunt.com/2-billion-email-addresses-were-exposed-and-we-indexed-them-all-in-have-i-been-pwned/</guid>
      <pubDate>Thu, 06 Nov 2025 20:20:23 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士编码了问题的难度</title>
      <link>https://arxiv.org/abs/2510.18147</link>
      <description><![CDATA[文章网址：https://arxiv.org/abs/2510.18147
评论网址：https://news.ycombinator.com/item?id=45838564
积分：94
# 条评论：17]]></description>
      <guid>https://arxiv.org/abs/2510.18147</guid>
      <pubDate>Thu, 06 Nov 2025 18:29:03 GMT</pubDate>
    </item>
    <item>
      <title>显示 HN：TabPFN-2.5 – 表格数据的 SOTA 基础模型</title>
      <link>https://priorlabs.ai/technical-reports/tabpfn-2-5-model-report</link>
      <description><![CDATA[我很高兴地宣布发布 TabPFN-2.5，这是我们的表格基础模型，现在可扩展到包含多达 50,000 个样本和 2,000 个特征的数据集，比今年早些时候在《自然》杂志上发布的 TabPFN v2 增加了 5 倍。 TabPFN-2.5 在一次前向传递中提供最先进的预测，无需跨分类和回归任务进行超参数调整。2.5 中的新增功能：
TabPFN-2.5 保留了 v2 的核心方法 - 一个预训练的 Transformer，在超过一亿个合成数据集上进行训练，以执行上下文学习并输出测试数据的预测分布。它本身支持缺失值、分类特征、文本和数字特征，对异常值和无信息特征具有鲁棒性。主要改进：- 规模增加 5 倍：现在可处理 50,000 个样本 × 2,000 个特征（v2 中为 10,000 × 500 个）- SOTA 性能：TabPFN-2.5 优于基于树的调整方法，并与复杂模型的性能相匹配ensemble (AutoGluon 1.4)，其本身包括 TabPFN v2，调整了 4 小时。调整模型可提高性能，在回归任务方面优于 AutoGluon 1.4。- 重建 API：新的 REST 接口以及具有专用拟合和预测端点的 Python SDK，使部署和集成对开发人员更加友好- 蒸馏引擎，可将 TabPFN-2.5 转换为紧凑的 MLP 或树集成，同时保持准确性并提供低延迟推理。仍然存在一些限制。该模型专为高达 50K 样本的数据集而设计。它可以处理更大的数据集，但这并不是 TabPFN-2.5 的重点。蒸馏引擎尚未通过 API 提供，只能通过许可证使用（尽管我们确实在模型报告中展示了性能）。我们正在积极努力消除这些限制，并打算发布专注于上下文推理、因果推理、图形网络、更大数据和时间序列的新模型。
TabPFN-2.5 可通过 API 和 Hugging Face 上的包获得。希望您尝试一下并向我们提供反馈！模型报告：https://priorlabs.ai/technical-reports/tabpfn-2-5-model-repo...包装：https://github.com/PriorLabs/TabPFN客户端：https://github.com/PriorLabs/tabpfn-client文档：https://docs.priorlabs.ai/quickstart
&lt;小时/&gt;
评论网址：https://news.ycombinator.com/item?id=45838540
积分：61
# 条评论：11]]></description>
      <guid>https://priorlabs.ai/technical-reports/tabpfn-2-5-model-report</guid>
      <pubDate>Thu, 06 Nov 2025 18:26:53 GMT</pubDate>
    </item>
    <item>
      <title>FreeBSD 上的 Swift 预览版</title>
      <link>https://forums.swift.org/t/swift-on-freebsd-preview/83064</link>
      <description><![CDATA[文章网址：https://forums.swift.org/t/swift-on-freebsd-preview/83064
评论网址：https://news.ycombinator.com/item?id=45837871
积分：175
# 条评论：101]]></description>
      <guid>https://forums.swift.org/t/swift-on-freebsd-preview/83064</guid>
      <pubDate>Thu, 06 Nov 2025 17:37:49 GMT</pubDate>
    </item>
    </channel>
</rss>